{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análisis de Datos - Machine Learning\n",
    "\n",
    "Este notebook contiene el análisis completo de datos con modelos de machine learning.\n",
    "\n",
    "## Pasos del análisis:\n",
    "1. Carga de datos\n",
    "2. Conversión a JSON y conteo por cuenta\n",
    "3. Análisis Exploratorio de Datos (EDA)\n",
    "4. Filtrado de datos (Tarifa NBO y Rentabilizacion)\n",
    "5. Preparación de datos\n",
    "6. Entrenamiento de modelos (Regresión Logística, Random Forest, XGBoost)\n",
    "7. Validación cruzada\n",
    "8. Grid Search para árboles de decisión\n",
    "9. Comparación y selección del mejor modelo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paso 1: Importación de librerías\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, accuracy_score\n",
    "import warnings\n",
    "import json\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"✓ Librerías importadas correctamente\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paso 2: Carga de datos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "archivo = \"Total_Mes_Act_Datos completos CORREGIDO.csv\"\n",
    "output_dir = 'resultados'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "print(\"Cargando datos...\")\n",
    "try:\n",
    "    df = pd.read_csv(\n",
    "        archivo, \n",
    "        low_memory=False,\n",
    "        encoding='utf-8',\n",
    "        on_bad_lines='skip',\n",
    "        sep=',',\n",
    "        quotechar='\"'\n",
    "    )\n",
    "    print(f\"✓ Datos cargados: {df.shape[0]} filas, {df.shape[1]} columnas\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    print(\"Intentando con encoding alternativo...\")\n",
    "    df = pd.read_csv(\n",
    "        archivo,\n",
    "        low_memory=False,\n",
    "        encoding='latin-1',\n",
    "        on_bad_lines='skip',\n",
    "        sep=';',\n",
    "        quotechar='\"'\n",
    "    )\n",
    "    print(f\"✓ Datos cargados: {df.shape[0]} filas, {df.shape[1]} columnas\")\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paso 3: Conversión a JSON y Conteo por Cuenta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Convirtiendo datos a JSON (muestra de 100,000 registros)...\")\n",
    "max_rows_json = min(100000, len(df))\n",
    "df_muestra = df.head(max_rows_json)\n",
    "\n",
    "ruta_json = os.path.join(output_dir, 'datos_completos.json')\n",
    "df_muestra.to_json(\n",
    "    ruta_json,\n",
    "    orient='records',\n",
    "    date_format='iso',\n",
    "    indent=2,\n",
    "    force_ascii=False\n",
    ")\n",
    "print(f\"✓ Archivo JSON guardado en '{ruta_json}'\")\n",
    "print(f\"Total de registros convertidos: {len(df_muestra)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columna_cuenta = None\n",
    "\n",
    "for col in df.columns:\n",
    "    if 'cuenta' in col.lower():\n",
    "        columna_cuenta = col\n",
    "        break\n",
    "\n",
    "if columna_cuenta is None:\n",
    "    posibles = [col for col in df.columns if 'cuent' in col.lower() or 'account' in col.lower()]\n",
    "    if posibles:\n",
    "        columna_cuenta = posibles[0]\n",
    "\n",
    "if columna_cuenta:\n",
    "    print(f\"Columna seleccionada: {columna_cuenta}\")\n",
    "    conteo = df[columna_cuenta].value_counts().reset_index()\n",
    "    conteo.columns = ['Cuenta', 'Frecuencia']\n",
    "    conteo = conteo.sort_values('Frecuencia', ascending=False)\n",
    "    \n",
    "    print(f\"\\nTotal de cuentas únicas: {len(conteo)}\")\n",
    "    print(f\"\\nTop 10 cuentas más frecuentes:\")\n",
    "    display(conteo.head(10))\n",
    "    \n",
    "    archivo_csv = os.path.join(output_dir, 'conteo_por_cuenta.csv')\n",
    "    conteo.to_csv(archivo_csv, index=False, encoding='utf-8')\n",
    "    print(f\"\\n✓ Conteo guardado en '{archivo_csv}'\")\n",
    "else:\n",
    "    print(\"No se encontró columna 'cuenta'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paso 4: Análisis Exploratorio de Datos (EDA)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Información del dataset:\")\n",
    "print(f\"Dimensiones: {df.shape}\")\n",
    "print(f\"\\nColumnas: {len(df.columns)}\")\n",
    "print(f\"\\nTipos de datos:\")\n",
    "print(df.dtypes.value_counts())\n",
    "\n",
    "print(f\"\\nValores faltantes:\")\n",
    "missing = df.isnull().sum()\n",
    "missing_pct = (missing / len(df)) * 100\n",
    "missing_df = pd.DataFrame({\n",
    "    'Valores_Faltantes': missing,\n",
    "    'Porcentaje': missing_pct\n",
    "})\n",
    "missing_df = missing_df[missing_df['Valores_Faltantes'] > 0].sort_values('Porcentaje', ascending=False)\n",
    "\n",
    "if len(missing_df) > 0:\n",
    "    display(missing_df.head(20))\n",
    "else:\n",
    "    print(\"No hay valores faltantes\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paso 5: Identificación y Filtrado de Datos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columna_tarifa = None\n",
    "columna_target = None\n",
    "\n",
    "for col in df.columns:\n",
    "    if 'tarifa' in col.lower() and 'nbo' in col.lower():\n",
    "        columna_tarifa = col\n",
    "        break\n",
    "\n",
    "if columna_tarifa is None:\n",
    "    posibles = [col for col in df.columns if 'tarifa' in col.lower() or 'nbo' in col.lower()]\n",
    "    if posibles:\n",
    "        columna_tarifa = posibles[0]\n",
    "\n",
    "for col in df.columns:\n",
    "    if 'rentabilizacion' in col.lower() or 'rentabiliz' in col.lower():\n",
    "        columna_target = col\n",
    "        break\n",
    "\n",
    "if columna_target is None:\n",
    "    posibles = [col for col in df.columns if 'rent' in col.lower()]\n",
    "    if posibles:\n",
    "        columna_target = posibles[0]\n",
    "\n",
    "print(f\"Columna Tarifa NBO: {columna_tarifa}\")\n",
    "print(f\"Columna Rentabilizacion: {columna_target}\")\n",
    "\n",
    "if columna_tarifa and columna_target:\n",
    "    df_filtrado = df[df[columna_tarifa].notna()].copy()\n",
    "    print(f\"\\nFilas después de filtrar por {columna_tarifa}: {len(df_filtrado)}\")\n",
    "    \n",
    "    df_filtrado = df_filtrado[df_filtrado[columna_target].notna()].copy()\n",
    "    print(f\"Filas después de filtrar por {columna_target}: {len(df_filtrado)}\")\n",
    "    \n",
    "    print(f\"\\nDistribución de la variable objetivo ({columna_target}):\")\n",
    "    display(df_filtrado[columna_target].value_counts())\n",
    "else:\n",
    "    print(\"\\nError: No se encontraron las columnas necesarias\")\n",
    "    df_filtrado = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paso 6: Preparación de Datos para Modelado\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df_filtrado is not None and columna_target:\n",
    "    y = df_filtrado[columna_target].copy()\n",
    "    X = df_filtrado.drop(columns=[columna_target])\n",
    "    \n",
    "    columnas_numericas = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    columnas_categoricas = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "    \n",
    "    print(f\"Columnas numéricas: {len(columnas_numericas)}\")\n",
    "    print(f\"Columnas categóricas: {len(columnas_categoricas)}\")\n",
    "    \n",
    "    X_processed = X[columnas_numericas].copy()\n",
    "    \n",
    "    for col in columnas_categoricas:\n",
    "        if X[col].nunique() < 50:\n",
    "            le = LabelEncoder()\n",
    "            X_processed[col] = le.fit_transform(X[col].astype(str).fillna('Missing'))\n",
    "    \n",
    "    X_processed = X_processed.fillna(X_processed.median())\n",
    "    \n",
    "    if isinstance(y.dtype, object) or y.dtype == 'object':\n",
    "        le_target = LabelEncoder()\n",
    "        y = le_target.fit_transform(y.astype(str))\n",
    "    else:\n",
    "        y = y.astype(int)\n",
    "    \n",
    "    print(f\"\\nShape final: X={X_processed.shape}, y={y.shape}\")\n",
    "    print(f\"Clases en y: {np.unique(y)}\")\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_processed, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n✓ Datos preparados:\")\n",
    "    print(f\"  Train: {X_train.shape[0]} muestras\")\n",
    "    print(f\"  Test: {X_test.shape[0]} muestras\")\n",
    "else:\n",
    "    print(\"Error: No se pueden preparar los datos\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paso 7: Entrenamiento de Modelos\n",
    "\n",
    "Dividimos el flujo en subpasos utilizando lecturas por bloques de 50.000 filas para monitorear el progreso y reutilizar los artefactos generados.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paso 8: Grid Search con muestreo estratificado\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'X_processed' not in globals() or 'y' not in globals():\n",
    "    raise RuntimeError('Es necesario ejecutar el Paso 7.1.1 antes de continuar.')\n",
    "\n",
    "n_muestra = min(300000, len(X_processed))\n",
    "print(f\"Tomando muestra estratificada de {n_muestra} registros para grid search...\", flush=True)\n",
    "\n",
    "X_muestra, _, y_muestra, _ = train_test_split(\n",
    "    X_processed,\n",
    "    y,\n",
    "    train_size=n_muestra,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "X_train_grid, X_test_grid, y_train_grid, y_test_grid = train_test_split(\n",
    "    X_muestra,\n",
    "    y_muestra,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y_muestra\n",
    ")\n",
    "\n",
    "print(f\"  Train: {X_train_grid.shape[0]} muestras\", flush=True)\n",
    "print(f\"  Test: {X_test_grid.shape[0]} muestras\", flush=True)\n",
    "\n",
    "cv_grid = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "print('\\nGrid Search - Random Forest', flush=True)\n",
    "parametros_rf = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [None, 20],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 2]\n",
    "}\n",
    "\n",
    "rf_base = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "modelo_grid_rf = GridSearchCV(\n",
    "    rf_base,\n",
    "    parametros_rf,\n",
    "    cv=cv_grid,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    verbose=2\n",
    ")\n",
    "modelo_grid_rf.fit(X_train_grid, y_train_grid)\n",
    "\n",
    "resultado_grid_rf = {\n",
    "    'best_params': modelo_grid_rf.best_params_,\n",
    "    'best_score': float(modelo_grid_rf.best_score_)\n",
    "}\n",
    "\n",
    "with open(os.path.join(output_dir, 'grid_rf.json'), 'w', encoding='utf-8') as archivo_rf:\n",
    "    json.dump(resultado_grid_rf, archivo_rf, indent=2)\n",
    "\n",
    "print(f\"✓ Mejor combinación RF: {resultado_grid_rf['best_params']} (score {resultado_grid_rf['best_score']:.4f})\", flush=True)\n",
    "\n",
    "print('\\nGrid Search - XGBoost', flush=True)\n",
    "parametros_xgb = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [3, 5],\n",
    "    'learning_rate': [0.1, 0.2],\n",
    "    'subsample': [0.8, 1.0]\n",
    "}\n",
    "\n",
    "xgb_base = XGBClassifier(random_state=42, n_jobs=-1, eval_metric='logloss')\n",
    "modelo_grid_xgb = GridSearchCV(\n",
    "    xgb_base,\n",
    "    parametros_xgb,\n",
    "    cv=cv_grid,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    verbose=2\n",
    ")\n",
    "modelo_grid_xgb.fit(X_train_grid, y_train_grid)\n",
    "\n",
    "resultado_grid_xgb = {\n",
    "    'best_params': modelo_grid_xgb.best_params_,\n",
    "    'best_score': float(modelo_grid_xgb.best_score_)\n",
    "}\n",
    "\n",
    "with open(os.path.join(output_dir, 'grid_xgb.json'), 'w', encoding='utf-8') as archivo_xgb:\n",
    "    json.dump(resultado_grid_xgb, archivo_xgb, indent=2)\n",
    "\n",
    "print(f\"✓ Mejor combinación XGBoost: {resultado_grid_xgb['best_params']} (score {resultado_grid_xgb['best_score']:.4f})\", flush=True)\n",
    "print('\\n✓ Paso 8 completado', flush=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paso 9: Resumen de resultados de Grid Search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruta_grid_rf = os.path.join(output_dir, 'grid_rf.json')\n",
    "ruta_grid_xgb = os.path.join(output_dir, 'grid_xgb.json')\n",
    "\n",
    "with open(ruta_grid_rf, 'r', encoding='utf-8') as archivo_rf:\n",
    "    resumen_rf = json.load(archivo_rf)\n",
    "\n",
    "with open(ruta_grid_xgb, 'r', encoding='utf-8') as archivo_xgb:\n",
    "    resumen_xgb = json.load(archivo_xgb)\n",
    "\n",
    "print('Resultados guardados de Grid Search:', flush=True)\n",
    "print(f\"  Random Forest -> score CV: {resumen_rf['best_score']:.4f} | parámetros: {resumen_rf['best_params']}\", flush=True)\n",
    "print(f\"  XGBoost       -> score CV: {resumen_xgb['best_score']:.4f} | parámetros: {resumen_xgb['best_params']}\", flush=True)\n",
    "\n",
    "if 'modelo_grid_rf' in globals() and 'modelo_grid_xgb' in globals():\n",
    "    print('\\nEvaluación rápida en el conjunto de prueba retenido de la muestra:', flush=True)\n",
    "    pred_rf = modelo_grid_rf.best_estimator_.predict(X_test_grid)\n",
    "    pred_xgb = modelo_grid_xgb.best_estimator_.predict(X_test_grid)\n",
    "    print(f\"  Random Forest -> accuracy test: {accuracy_score(y_test_grid, pred_rf):.4f}\", flush=True)\n",
    "    print(f\"  XGBoost       -> accuracy test: {accuracy_score(y_test_grid, pred_xgb):.4f}\", flush=True)\n",
    "else:\n",
    "    print('\\nPara evaluar en test es necesario volver a ejecutar el Paso 8 en esta sesión.', flush=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paso 10: Comparación y Conclusión\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruta_grid_rf = os.path.join(output_dir, 'grid_rf.json')\n",
    "ruta_grid_xgb = os.path.join(output_dir, 'grid_xgb.json')\n",
    "ruta_cv_lr = os.path.join(output_dir, 'resultado_cv_lr.json')\n",
    "\n",
    "faltantes = [ruta for ruta in [ruta_grid_rf, ruta_grid_xgb, ruta_cv_lr] if not os.path.exists(ruta)]\n",
    "if faltantes:\n",
    "    raise FileNotFoundError(f\"No se encontraron los archivos requeridos: {faltantes}\")\n",
    "\n",
    "with open(ruta_grid_rf, 'r', encoding='utf-8') as archivo_rf:\n",
    "    resumen_rf = json.load(archivo_rf)\n",
    "\n",
    "with open(ruta_grid_xgb, 'r', encoding='utf-8') as archivo_xgb:\n",
    "    resumen_xgb = json.load(archivo_xgb)\n",
    "\n",
    "with open(ruta_cv_lr, 'r', encoding='utf-8') as archivo_lr:\n",
    "    resumen_lr = json.load(archivo_lr)\n",
    "\n",
    "print('COMPARACIÓN DE MODELOS', flush=True)\n",
    "print('=' * 80, flush=True)\n",
    "print('\\nValidación cruzada - Regresión logística:', flush=True)\n",
    "print(f\"  Accuracy promedio: {resumen_lr['mean']:.4f} (+/- {resumen_lr['std'] * 2:.4f})\", flush=True)\n",
    "\n",
    "print('\\nGrid Search - Modelos de árboles:', flush=True)\n",
    "print(f\"  Random Forest -> score CV: {resumen_rf['best_score']:.4f} | parámetros: {resumen_rf['best_params']}\", flush=True)\n",
    "print(f\"  XGBoost       -> score CV: {resumen_xgb['best_score']:.4f} | parámetros: {resumen_xgb['best_params']}\", flush=True)\n",
    "\n",
    "mejor_arbol = 'RandomForest' if resumen_rf['best_score'] > resumen_xgb['best_score'] else 'XGBoost'\n",
    "\n",
    "print('\\n' + '=' * 80, flush=True)\n",
    "print('CONCLUSIÓN', flush=True)\n",
    "print('=' * 80, flush=True)\n",
    "print(f\"  Mejor modelo entre árboles (según Grid Search): {mejor_arbol}\", flush=True)\n",
    "\n",
    "conclusion = {\n",
    "    'cv_lr_mean': resumen_lr['mean'],\n",
    "    'cv_lr_std': resumen_lr['std'],\n",
    "    'grid_rf': resumen_rf,\n",
    "    'grid_xgb': resumen_xgb,\n",
    "    'best_tree_model': mejor_arbol\n",
    "}\n",
    "\n",
    "ruta_conclusion = os.path.join(output_dir, 'conclusion_paso_10.json')\n",
    "with open(ruta_conclusion, 'w', encoding='utf-8') as archivo_conclusion:\n",
    "    json.dump(conclusion, archivo_conclusion, indent=2)\n",
    "\n",
    "print(f\"\\n✓ Conclusión almacenada en: {ruta_conclusion}\", flush=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paso 11: Análisis de Importancia de Variables y Visualizaciones\n",
    "\n",
    "En este paso analizamos las variables más importantes usando el mejor modelo (XGBoost) y creamos visualizaciones del análisis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from xgboost import plot_tree, plot_importance\n",
    "import graphviz\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc, precision_recall_curve\n",
    "from sklearn.tree import export_graphviz\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print('=' * 80)\n",
    "print('PASO 11: ANÁLISIS DE IMPORTANCIA Y VISUALIZACIONES')\n",
    "print('=' * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nCargando datos preparados y resultados...', flush=True)\n",
    "\n",
    "ruta_datos = os.path.join(output_dir, 'datos_preparados_7_1.pkl')\n",
    "with open(ruta_datos, 'rb') as f:\n",
    "    datos = pickle.load(f)\n",
    "\n",
    "X_train_scaled = datos['X_train_scaled']\n",
    "X_test_scaled = datos['X_test_scaled']\n",
    "y_train = datos['y_train']\n",
    "y_test = datos['y_test']\n",
    "\n",
    "print(f'✓ Datos cargados: Train={X_train_scaled.shape[0]}, Test={X_test_scaled.shape[0]}', flush=True)\n",
    "\n",
    "with open(os.path.join(output_dir, 'grid_xgb.json'), 'r', encoding='utf-8') as f:\n",
    "    grid_xgb_params = json.load(f)\n",
    "\n",
    "print(f'✓ Parámetros óptimos XGBoost cargados: {grid_xgb_params[\"best_params\"]}', flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nEntrenando modelo XGBoost con parámetros óptimos...', flush=True)\n",
    "\n",
    "xgb_optimizado = XGBClassifier(\n",
    "    n_estimators=grid_xgb_params['best_params']['n_estimators'],\n",
    "    max_depth=grid_xgb_params['best_params']['max_depth'],\n",
    "    learning_rate=grid_xgb_params['best_params']['learning_rate'],\n",
    "    subsample=grid_xgb_params['best_params']['subsample'],\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    eval_metric='logloss'\n",
    ")\n",
    "\n",
    "xgb_optimizado.fit(X_train_scaled, y_train)\n",
    "\n",
    "y_pred_xgb = xgb_optimizado.predict(X_test_scaled)\n",
    "y_pred_proba_xgb = xgb_optimizado.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "accuracy_xgb = accuracy_score(y_test, y_pred_xgb)\n",
    "print(f'✓ Modelo entrenado - Accuracy en test: {accuracy_xgb:.4f}', flush=True)\n",
    "\n",
    "ruta_modelo_xgb = os.path.join(output_dir, 'modelo_xgb_optimizado.pkl')\n",
    "with open(ruta_modelo_xgb, 'wb') as f:\n",
    "    pickle.dump(xgb_optimizado, f)\n",
    "print(f'✓ Modelo guardado en: {ruta_modelo_xgb}', flush=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.1: Importancia de Características\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nAnalizando importancia de características...', flush=True)\n",
    "\n",
    "importancia = xgb_optimizado.feature_importances_\n",
    "feature_names = [f'Feature_{i}' for i in range(len(importancia))]\n",
    "\n",
    "df_importancia = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': importancia\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(f'\\nTop 20 características más importantes:', flush=True)\n",
    "print(df_importancia.head(20).to_string(index=False), flush=True)\n",
    "\n",
    "df_importancia.to_csv(os.path.join(output_dir, 'importancia_caracteristicas.csv'), index=False)\n",
    "print(f'\\n✓ Importancia guardada en CSV', flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(20, 16))\n",
    "\n",
    "top_n = 20\n",
    "df_top = df_importancia.head(top_n)\n",
    "\n",
    "axes[0, 0].barh(range(len(df_top)), df_top['Importance'].values, color='steelblue')\n",
    "axes[0, 0].set_yticks(range(len(df_top)))\n",
    "axes[0, 0].set_yticklabels(df_top['Feature'].values)\n",
    "axes[0, 0].set_xlabel('Importancia', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].set_title(f'Top {top_n} Características Más Importantes (XGBoost)', \n",
    "                     fontsize=14, fontweight='bold', pad=20)\n",
    "axes[0, 0].invert_yaxis()\n",
    "axes[0, 0].grid(axis='x', alpha=0.3)\n",
    "\n",
    "axes[0, 1].bar(range(len(df_top)), df_top['Importance'].values, color='coral')\n",
    "axes[0, 1].set_xticks(range(len(df_top)))\n",
    "axes[0, 1].set_xticklabels(df_top['Feature'].values, rotation=45, ha='right')\n",
    "axes[0, 1].set_ylabel('Importancia', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].set_title(f'Top {top_n} Características - Vista de Barras', \n",
    "                     fontsize=14, fontweight='bold', pad=20)\n",
    "axes[0, 1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "importancia_acumulada = df_importancia['Importance'].cumsum()\n",
    "axes[1, 0].plot(range(1, len(importancia_acumulada) + 1), importancia_acumulada.values, \n",
    "                marker='o', linewidth=2, markersize=4, color='green')\n",
    "axes[1, 0].axhline(y=0.8, color='r', linestyle='--', label='80% de importancia')\n",
    "axes[1, 0].set_xlabel('Número de Características', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].set_ylabel('Importancia Acumulada', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].set_title('Importancia Acumulada de Características', \n",
    "                     fontsize=14, fontweight='bold', pad=20)\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(alpha=0.3)\n",
    "\n",
    "n_features_80 = len(importancia_acumulada[importancia_acumulada <= 0.8])\n",
    "axes[1, 0].axvline(x=n_features_80, color='orange', linestyle='--', \n",
    "                   label=f'{n_features_80} features = 80%')\n",
    "axes[1, 0].legend()\n",
    "\n",
    "axes[1, 1].pie(df_top['Importance'].values, labels=df_top['Feature'].values, \n",
    "               autopct='%1.1f%%', startangle=90)\n",
    "axes[1, 1].set_title(f'Distribución de Importancia - Top {top_n}', \n",
    "                     fontsize=14, fontweight='bold', pad=20)\n",
    "\n",
    "plt.tight_layout()\n",
    "ruta_grafica = os.path.join(output_dir, 'importancia_caracteristicas.png')\n",
    "plt.savefig(ruta_grafica, dpi=300, bbox_inches='tight')\n",
    "print(f'✓ Gráfica de importancia guardada en: {ruta_grafica}', flush=True)\n",
    "plt.show()\n",
    "\n",
    "print(f'\\nNúmero de características que explican el 80% de la importancia: {n_features_80}', flush=True)\n",
    "print(f'Características clave (top {n_features_80}):', flush=True)\n",
    "for i, row in df_importancia.head(n_features_80).iterrows():\n",
    "    print(f\"  {row['Feature']}: {row['Importance']:.6f}\", flush=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.2: Visualización de Árboles de Decisión\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nVisualizando árboles de decisión individuales...', flush=True)\n",
    "\n",
    "try:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(30, 24))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, ax in enumerate(axes[:4]):\n",
    "        plot_tree(xgb_optimizado, num_trees=i, ax=ax, rankdir='LR')\n",
    "        ax.set_title(f'Árbol de Decisión #{i+1} (XGBoost)', fontsize=12, fontweight='bold', pad=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    ruta_arboles = os.path.join(output_dir, 'arboles_decision_xgb.png')\n",
    "    plt.savefig(ruta_arboles, dpi=300, bbox_inches='tight')\n",
    "    print(f'✓ Árboles de decisión guardados en: {ruta_arboles}', flush=True)\n",
    "    plt.show()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f'⚠ No se pudieron visualizar los árboles con plot_tree: {e}', flush=True)\n",
    "    print('Intentando método alternativo...', flush=True)\n",
    "    \n",
    "    try:\n",
    "        from xgboost import to_graphviz\n",
    "        \n",
    "        for i in range(min(3, xgb_optimizado.n_estimators)):\n",
    "            graph = to_graphviz(xgb_optimizado, num_trees=i)\n",
    "            ruta_arbol = os.path.join(output_dir, f'arbol_decision_{i+1}.dot')\n",
    "            graph.save(ruta_arbol)\n",
    "            print(f'✓ Árbol {i+1} guardado en formato DOT: {ruta_arbol}', flush=True)\n",
    "    except Exception as e2:\n",
    "        print(f'⚠ Método alternativo también falló: {e2}', flush=True)\n",
    "        print('Los árboles están disponibles en el modelo pero requieren herramientas adicionales para visualización.', flush=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.3: Matriz de Confusión y Métricas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred_xgb)\n",
    "precision = precision_score(y_test, y_pred_xgb)\n",
    "recall = recall_score(y_test, y_pred_xgb)\n",
    "f1 = f1_score(y_test, y_pred_xgb)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba_xgb)\n",
    "\n",
    "print('\\n' + '=' * 80)\n",
    "print('MÉTRICAS DEL MODELO XGBOOST')\n",
    "print('=' * 80)\n",
    "print(f'Accuracy:  {accuracy_xgb:.4f}')\n",
    "print(f'Precision: {precision:.4f}')\n",
    "print(f'Recall:    {recall:.4f}')\n",
    "print(f'F1-Score:  {f1:.4f}')\n",
    "print(f'ROC-AUC:   {roc_auc:.4f}')\n",
    "print('=' * 80)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0], \n",
    "            xticklabels=['Clase 0', 'Clase 1'], \n",
    "            yticklabels=['Clase 0', 'Clase 1'])\n",
    "axes[0].set_xlabel('Predicción', fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylabel('Real', fontsize=12, fontweight='bold')\n",
    "axes[0].set_title('Matriz de Confusión', fontsize=14, fontweight='bold', pad=20)\n",
    "\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "axes[1].bar(['Verdaderos\\nNegativos', 'Falsos\\nPositivos', 'Falsos\\nNegativos', 'Verdaderos\\nPositivos'],\n",
    "            [tn, fp, fn, tp], color=['green', 'orange', 'red', 'blue'], alpha=0.7)\n",
    "axes[1].set_ylabel('Cantidad', fontsize=12, fontweight='bold')\n",
    "axes[1].set_title('Desglose de la Matriz de Confusión', fontsize=14, fontweight='bold', pad=20)\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "for i, v in enumerate([tn, fp, fn, tp]):\n",
    "    axes[1].text(i, v + max([tn, fp, fn, tp]) * 0.01, str(v), \n",
    "                ha='center', va='bottom', fontweight='bold', fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "ruta_confusion = os.path.join(output_dir, 'matriz_confusion_xgb.png')\n",
    "plt.savefig(ruta_confusion, dpi=300, bbox_inches='tight')\n",
    "print(f'\\n✓ Matriz de confusión guardada en: {ruta_confusion}', flush=True)\n",
    "plt.show()\n",
    "\n",
    "reporte = classification_report(y_test, y_pred_xgb, output_dict=True)\n",
    "df_reporte = pd.DataFrame(reporte).transpose()\n",
    "df_reporte.to_csv(os.path.join(output_dir, 'reporte_clasificacion_xgb.csv'))\n",
    "print('✓ Reporte de clasificación guardado en CSV', flush=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.4: Curvas ROC y Precision-Recall\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, _ = roc_curve(y_test, y_pred_proba_xgb)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "precision_curve, recall_curve, _ = precision_recall_curve(y_test, y_pred_proba_xgb)\n",
    "pr_auc = auc(recall_curve, precision_curve)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "axes[0].plot(fpr, tpr, color='darkorange', lw=2, \n",
    "             label=f'ROC curve (AUC = {roc_auc:.4f})')\n",
    "axes[0].plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random Classifier')\n",
    "axes[0].set_xlabel('Tasa de Falsos Positivos', fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylabel('Tasa de Verdaderos Positivos', fontsize=12, fontweight='bold')\n",
    "axes[0].set_title('Curva ROC', fontsize=14, fontweight='bold', pad=20)\n",
    "axes[0].legend(loc=\"lower right\", fontsize=11)\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "axes[1].plot(recall_curve, precision_curve, color='blue', lw=2,\n",
    "             label=f'Precision-Recall curve (AUC = {pr_auc:.4f})')\n",
    "baseline = len(y_test[y_test==1]) / len(y_test)\n",
    "axes[1].axhline(y=baseline, color='r', linestyle='--', \n",
    "                label=f'Baseline ({baseline:.4f})')\n",
    "axes[1].set_xlabel('Recall', fontsize=12, fontweight='bold')\n",
    "axes[1].set_ylabel('Precision', fontsize=12, fontweight='bold')\n",
    "axes[1].set_title('Curva Precision-Recall', fontsize=14, fontweight='bold', pad=20)\n",
    "axes[1].legend(loc=\"lower left\", fontsize=11)\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "ruta_curvas = os.path.join(output_dir, 'curvas_roc_precision_recall.png')\n",
    "plt.savefig(ruta_curvas, dpi=300, bbox_inches='tight')\n",
    "print(f'✓ Curvas ROC y Precision-Recall guardadas en: {ruta_curvas}', flush=True)\n",
    "plt.show()\n",
    "\n",
    "print(f'\\nÁrea bajo la curva ROC: {roc_auc:.4f}', flush=True)\n",
    "print(f'Área bajo la curva Precision-Recall: {pr_auc:.4f}', flush=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.5: Resumen de Variables Clave\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n' + '=' * 80)\n",
    "print('RESUMEN DE VARIABLES CLAVE PARA EL MODELO')\n",
    "print('=' * 80)\n",
    "\n",
    "n_variables_clave = min(15, len(df_importancia))\n",
    "variables_clave = df_importancia.head(n_variables_clave)\n",
    "\n",
    "print(f'\\nLas {n_variables_clave} variables más importantes son:\\n', flush=True)\n",
    "for idx, row in variables_clave.iterrows():\n",
    "    porcentaje = (row['Importance'] / df_importancia['Importance'].sum()) * 100\n",
    "    print(f\"  {idx+1:2d}. {row['Feature']:20s} - Importancia: {row['Importance']:.6f} ({porcentaje:.2f}%)\", flush=True)\n",
    "\n",
    "importancia_total_top = variables_clave['Importance'].sum()\n",
    "porcentaje_total = (importancia_total_top / df_importancia['Importance'].sum()) * 100\n",
    "print(f'\\nEstas {n_variables_clave} variables explican el {porcentaje_total:.2f}% de la importancia total.', flush=True)\n",
    "\n",
    "resumen_variables = {\n",
    "    'variables_clave': variables_clave['Feature'].tolist(),\n",
    "    'importancia_total': float(importancia_total_top),\n",
    "    'porcentaje_explicado': float(porcentaje_total),\n",
    "    'total_variables': len(df_importancia),\n",
    "    'variables_seleccionadas': n_variables_clave\n",
    "}\n",
    "\n",
    "ruta_resumen = os.path.join(output_dir, 'resumen_variables_clave.json')\n",
    "with open(ruta_resumen, 'w', encoding='utf-8') as f:\n",
    "    json.dump(resumen_variables, f, indent=2)\n",
    "\n",
    "print(f'\\n✓ Resumen de variables clave guardado en: {ruta_resumen}', flush=True)\n",
    "print('\\n✓ Paso 11 completado exitosamente', flush=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paso 12: Visualización de Árboles de Decisión (Estructura)\n",
    "\n",
    "Se generan imágenes de la estructura de los árboles de decisión:\n",
    "- Intento 1: Árboles internos del modelo XGBoost (si hay soporte de Graphviz).\n",
    "- Intento 2: Árbol surrogado (sklearn) entrenado sobre las variables más importantes, exportado como PNG.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Generando visualizaciones de árboles...', flush=True)\n",
    "\n",
    "# Asegurar df_importancia y nombres de features\n",
    "ruta_importancias = os.path.join(output_dir, 'importancia_caracteristicas.csv')\n",
    "if 'df_importancia' not in globals():\n",
    "    if os.path.exists(ruta_importancias):\n",
    "        df_importancia = pd.read_csv(ruta_importancias)\n",
    "    else:\n",
    "        raise RuntimeError('No se encuentra df_importancia. Ejecute el Paso 11 primero.')\n",
    "\n",
    "# Intento 1: Visualizar árboles de XGBoost directamente (requiere Graphviz)\n",
    "exitos_xgb = False\n",
    "try:\n",
    "    from xgboost import plot_tree as xgb_plot_tree\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(30, 24))\n",
    "    axes = axes.flatten()\n",
    "    for i, ax in enumerate(axes[:4]):\n",
    "        xgb_plot_tree(xgb_optimizado, num_trees=i, ax=ax, rankdir='LR')\n",
    "        ax.set_title(f'Estructura - Árbol XGBoost #{i+1}', fontsize=12, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    ruta_arboles_xgb = os.path.join(output_dir, 'estructura_arboles_xgb.png')\n",
    "    plt.savefig(ruta_arboles_xgb, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(f'✓ Estructuras XGBoost guardadas en: {ruta_arboles_xgb}', flush=True)\n",
    "    exitos_xgb = True\n",
    "except Exception as e:\n",
    "    print(f'Nota: no fue posible visualizar árboles XGBoost directamente ({e}). Se usa árbol surrogado.', flush=True)\n",
    "\n",
    "# Intento 2: Árbol surrogado con sklearn sobre top-N variables más importantes\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree as sk_plot_tree\n",
    "\n",
    "# Tomar top-N variables\n",
    "top_n = 10\n",
    "indices_top = []\n",
    "for feat in df_importancia['Feature'].head(top_n).tolist():\n",
    "    # 'Feature_i' -> i\n",
    "    try:\n",
    "        idx = int(feat.split('_')[1])\n",
    "    except Exception:\n",
    "        idx = 0\n",
    "    indices_top.append(idx)\n",
    "\n",
    "# Preparar subconjunto de datos (usar escalados por consistencia)\n",
    "X_train_top = X_train_scaled[:, indices_top]\n",
    "X_test_top = X_test_scaled[:, indices_top]\n",
    "\n",
    "arbol_surrogado = DecisionTreeClassifier(max_depth=4, random_state=42)\n",
    "arbol_surrogado.fit(X_train_top, y_train)\n",
    "\n",
    "fig = plt.figure(figsize=(24, 14))\n",
    "sk_plot_tree(\n",
    "    arbol_surrogado,\n",
    "    feature_names=[df_importancia['Feature'].iloc[i] for i in range(top_n)],\n",
    "    class_names=['Clase 0', 'Clase 1'],\n",
    "    filled=True,\n",
    "    rounded=True,\n",
    "    fontsize=10\n",
    ")\n",
    "plt.title('Árbol de Decisión Surrogado (Top 10 variables)', fontsize=14, fontweight='bold')\n",
    "ruta_arbol_surrogado = os.path.join(output_dir, 'arbol_surrogado_top10.png')\n",
    "plt.savefig(ruta_arbol_surrogado, dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f'✓ Árbol surrogado guardado en: {ruta_arbol_surrogado}', flush=True)\n",
    "\n",
    "# Exportar también una versión más pequeña (profundidad 3) para lectura rápida\n",
    "arbol_surrogado_peq = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
    "arbol_surrogado_peq.fit(X_train_top, y_train)\n",
    "fig = plt.figure(figsize=(20, 12))\n",
    "sk_plot_tree(\n",
    "    arbol_surrogado_peq,\n",
    "    feature_names=[df_importancia['Feature'].iloc[i] for i in range(top_n)],\n",
    "    class_names=['Clase 0', 'Clase 1'],\n",
    "    filled=True,\n",
    "    rounded=True,\n",
    "    fontsize=10\n",
    ")\n",
    "plt.title('Árbol Surrogado (Profundidad 3) - Top 10 variables', fontsize=14, fontweight='bold')\n",
    "ruta_arbol_surrogado_peq = os.path.join(output_dir, 'arbol_surrogado_top10_depth3.png')\n",
    "plt.savefig(ruta_arbol_surrogado_peq, dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f'✓ Árbol surrogado (profundidad 3) guardado en: {ruta_arbol_surrogado_peq}', flush=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.1: Visualizaciones EDA Completas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('=' * 80)\n",
    "print('VISUALIZACIONES EDA COMPLETAS')\n",
    "print('=' * 80)\n",
    "\n",
    "print('\\nCargando datos originales para EDA...', flush=True)\n",
    "\n",
    "archivo = 'Total_Mes_Act_Datos completos CORREGIDO.csv'\n",
    "df_eda = pd.read_csv(\n",
    "    archivo, \n",
    "    low_memory=False,\n",
    "    encoding='utf-8',\n",
    "    on_bad_lines='skip',\n",
    "    sep=';',\n",
    "    quotechar='\"'\n",
    ")\n",
    "\n",
    "print(f'✓ Datos cargados: {df_eda.shape[0]} filas, {df_eda.shape[1]} columnas', flush=True)\n",
    "\n",
    "columna_tarifa = 'TARIFA_NBO'\n",
    "columna_rentabilizacion = 'Rentabilizo'\n",
    "\n",
    "df_eda['variable_objetivo'] = np.where(\n",
    "    (df_eda[columna_tarifa].notna()) & \n",
    "    (df_eda[columna_rentabilizacion].notna()),\n",
    "    1,\n",
    "    0\n",
    ")\n",
    "\n",
    "fig = plt.figure(figsize=(20, 16))\n",
    "\n",
    "distribucion = df_eda['variable_objetivo'].value_counts()\n",
    "ax1 = plt.subplot(3, 3, 1)\n",
    "distribucion.plot(kind='bar', color=['coral', 'steelblue'], ax=ax1)\n",
    "ax1.set_title('Distribución de Variable Objetivo', fontsize=12, fontweight='bold', pad=15)\n",
    "ax1.set_xlabel('Clase (0=No cumple, 1=Cumple)', fontsize=10, fontweight='bold')\n",
    "ax1.set_ylabel('Frecuencia', fontsize=10, fontweight='bold')\n",
    "ax1.set_xticklabels(['No cumple', 'Cumple'], rotation=0)\n",
    "for i, v in enumerate(distribucion.values):\n",
    "    ax1.text(i, v + max(distribucion.values) * 0.01, f'{v:,}', \n",
    "            ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "ax2 = plt.subplot(3, 3, 2)\n",
    "distribucion.plot(kind='pie', autopct='%1.1f%%', colors=['coral', 'steelblue'], ax=ax2, startangle=90)\n",
    "ax2.set_title('Distribución Porcentual', fontsize=12, fontweight='bold', pad=15)\n",
    "ax2.set_ylabel('')\n",
    "\n",
    "missing = df_eda.isnull().sum()\n",
    "missing_pct = (missing / len(df_eda)) * 100\n",
    "missing_df = pd.DataFrame({\n",
    "    'Valores_Faltantes': missing,\n",
    "    'Porcentaje': missing_pct\n",
    "})\n",
    "missing_df = missing_df[missing_df['Valores_Faltantes'] > 0].sort_values('Porcentaje', ascending=False).head(15)\n",
    "\n",
    "ax3 = plt.subplot(3, 3, 3)\n",
    "missing_df['Porcentaje'].plot(kind='barh', color='orange', ax=ax3)\n",
    "ax3.set_title('Top 15 Columnas con Valores Faltantes', fontsize=12, fontweight='bold', pad=15)\n",
    "ax3.set_xlabel('Porcentaje de Valores Faltantes', fontsize=10, fontweight='bold')\n",
    "ax3.invert_yaxis()\n",
    "\n",
    "columnas_numericas = df_eda.select_dtypes(include=[np.number]).columns.tolist()\n",
    "if len(columnas_numericas) > 0:\n",
    "    df_numericas = df_eda[columnas_numericas[:6]].copy()\n",
    "    \n",
    "    ax4 = plt.subplot(3, 3, 4)\n",
    "    df_numericas.boxplot(ax=ax4, rot=45)\n",
    "    ax4.set_title('Distribución de Variables Numéricas (Top 6)', fontsize=12, fontweight='bold', pad=15)\n",
    "    ax4.set_ylabel('Valor', fontsize=10, fontweight='bold')\n",
    "    \n",
    "    ax5 = plt.subplot(3, 3, 5)\n",
    "    df_numericas.hist(bins=30, ax=ax5, layout=(2, 3), figsize=(15, 10))\n",
    "    ax5.set_title('Histogramas de Variables Numéricas', fontsize=12, fontweight='bold', pad=15)\n",
    "\n",
    "if columna_tarifa in df_eda.columns:\n",
    "    ax6 = plt.subplot(3, 3, 6)\n",
    "    df_eda[columna_tarifa].notna().value_counts().plot(kind='bar', color=['red', 'green'], ax=ax6)\n",
    "    ax6.set_title(f'Distribución de {columna_tarifa}', fontsize=12, fontweight='bold', pad=15)\n",
    "    ax6.set_xlabel('Tiene Valor', fontsize=10, fontweight='bold')\n",
    "    ax6.set_ylabel('Frecuencia', fontsize=10, fontweight='bold')\n",
    "    ax6.set_xticklabels(['Faltante', 'Presente'], rotation=0)\n",
    "\n",
    "if columna_rentabilizacion in df_eda.columns:\n",
    "    ax7 = plt.subplot(3, 3, 7)\n",
    "    df_eda[columna_rentabilizacion].notna().value_counts().plot(kind='bar', color=['red', 'green'], ax=ax7)\n",
    "    ax7.set_title(f'Distribución de {columna_rentabilizacion}', fontsize=12, fontweight='bold', pad=15)\n",
    "    ax7.set_xlabel('Tiene Valor', fontsize=10, fontweight='bold')\n",
    "    ax7.set_ylabel('Frecuencia', fontsize=10, fontweight='bold')\n",
    "    ax7.set_xticklabels(['Faltante', 'Presente'], rotation=0)\n",
    "\n",
    "tipos_datos = df_eda.dtypes.value_counts()\n",
    "ax8 = plt.subplot(3, 3, 8)\n",
    "tipos_datos.plot(kind='bar', color='purple', ax=ax8)\n",
    "ax8.set_title('Distribución de Tipos de Datos', fontsize=12, fontweight='bold', pad=15)\n",
    "ax8.set_xlabel('Tipo de Dato', fontsize=10, fontweight='bold')\n",
    "ax8.set_ylabel('Cantidad de Columnas', fontsize=10, fontweight='bold')\n",
    "ax8.tick_params(axis='x', rotation=45)\n",
    "\n",
    "ax9 = plt.subplot(3, 3, 9)\n",
    "resumen_stats = {\n",
    "    'Total Filas': len(df_eda),\n",
    "    'Total Columnas': len(df_eda.columns),\n",
    "    'Columnas Numéricas': len(columnas_numericas),\n",
    "    'Columnas Categóricas': len(df_eda.columns) - len(columnas_numericas),\n",
    "    'Clase 1 (Cumple)': int(distribucion.get(1, 0)),\n",
    "    'Clase 0 (No cumple)': int(distribucion.get(0, 0))\n",
    "}\n",
    "stats_df = pd.DataFrame(list(resumen_stats.items()), columns=['Métrica', 'Valor'])\n",
    "stats_df.set_index('Métrica')['Valor'].plot(kind='barh', color='teal', ax=ax9)\n",
    "ax9.set_title('Resumen Estadístico del Dataset', fontsize=12, fontweight='bold', pad=15)\n",
    "ax9.set_xlabel('Valor', fontsize=10, fontweight='bold')\n",
    "ax9.invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "ruta_eda_completo = os.path.join(output_dir, 'eda_visualizaciones_completas.png')\n",
    "plt.savefig(ruta_eda_completo, dpi=300, bbox_inches='tight')\n",
    "print(f'\\n✓ Visualizaciones EDA completas guardadas en: {ruta_eda_completo}', flush=True)\n",
    "plt.show()\n",
    "\n",
    "print('\\n✓ Visualizaciones EDA completadas', flush=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.2: Árboles de Decisión Individuales con Variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('=' * 80)\n",
    "print('GENERANDO ÁRBOLES DE DECISIÓN INDIVIDUALES')\n",
    "print('=' * 80)\n",
    "\n",
    "print('\\nCargando datos y modelo...', flush=True)\n",
    "\n",
    "if 'df_importancia' not in globals():\n",
    "    ruta_importancias = os.path.join(output_dir, 'importancia_caracteristicas.csv')\n",
    "    if os.path.exists(ruta_importancias):\n",
    "        df_importancia = pd.read_csv(ruta_importancias)\n",
    "    else:\n",
    "        raise RuntimeError('Ejecute el Paso 11 primero para generar importancias.')\n",
    "\n",
    "if 'xgb_optimizado' not in globals():\n",
    "    ruta_modelo = os.path.join(output_dir, 'modelo_xgb_optimizado.pkl')\n",
    "    if os.path.exists(ruta_modelo):\n",
    "        with open(ruta_modelo, 'rb') as f:\n",
    "            xgb_optimizado = pickle.load(f)\n",
    "    else:\n",
    "        raise RuntimeError('Ejecute el Paso 11 primero para generar el modelo.')\n",
    "\n",
    "if 'X_train_scaled' not in globals() or 'y_train' not in globals():\n",
    "    ruta_datos = os.path.join(output_dir, 'datos_preparados_7_1.pkl')\n",
    "    with open(ruta_datos, 'rb') as f:\n",
    "        datos = pickle.load(f)\n",
    "    X_train_scaled = datos['X_train_scaled']\n",
    "    y_train = datos['y_train']\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "\n",
    "top_n_vars = 10\n",
    "top_features = df_importancia.head(top_n_vars)['Feature'].tolist()\n",
    "\n",
    "indices_top = []\n",
    "for feat in top_features:\n",
    "    try:\n",
    "        idx = int(feat.split('_')[1])\n",
    "        indices_top.append(idx)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "if len(indices_top) == 0:\n",
    "    indices_top = list(range(min(top_n_vars, X_train_scaled.shape[1])))\n",
    "\n",
    "X_train_top = X_train_scaled[:, indices_top]\n",
    "feature_names_top = [df_importancia.iloc[i]['Feature'] for i in range(len(indices_top))]\n",
    "\n",
    "print(f'\\nGenerando árboles individuales con top {top_n_vars} variables...', flush=True)\n",
    "print(f'Variables utilizadas: {feature_names_top}', flush=True)\n",
    "\n",
    "profundidades = [3, 4, 5]\n",
    "for depth in profundidades:\n",
    "    arbol = DecisionTreeClassifier(max_depth=depth, random_state=42, min_samples_split=100)\n",
    "    arbol.fit(X_train_top, y_train)\n",
    "    \n",
    "    fig = plt.figure(figsize=(24, 14))\n",
    "    plot_tree(\n",
    "        arbol,\n",
    "        feature_names=feature_names_top,\n",
    "        class_names=['Clase 0\\n(No cumple)', 'Clase 1\\n(Cumple)'],\n",
    "        filled=True,\n",
    "        rounded=True,\n",
    "        fontsize=9,\n",
    "        proportion=True\n",
    "    )\n",
    "    plt.title(f'Árbol de Decisión - Profundidad {depth} - Top {top_n_vars} Variables\\nVariables: {\", \".join(feature_names_top[:5])}...', \n",
    "              fontsize=14, fontweight='bold', pad=20)\n",
    "    \n",
    "    ruta_arbol = os.path.join(output_dir, f'arbol_decision_depth{depth}_top{top_n_vars}.png')\n",
    "    plt.savefig(ruta_arbol, dpi=300, bbox_inches='tight')\n",
    "    print(f'✓ Árbol (profundidad {depth}) guardado en: {ruta_arbol}', flush=True)\n",
    "    plt.close()\n",
    "\n",
    "print('\\nGenerando árboles individuales por variable más importante...', flush=True)\n",
    "\n",
    "for i, (idx, row) in enumerate(df_importancia.head(5).iterrows()):\n",
    "    feat_name = row['Feature']\n",
    "    try:\n",
    "        feat_idx = int(feat_name.split('_')[1])\n",
    "    except:\n",
    "        feat_idx = i\n",
    "    \n",
    "    if feat_idx >= X_train_scaled.shape[1]:\n",
    "        continue\n",
    "    \n",
    "    indices_single = [feat_idx] + [j for j in indices_top[:4] if j != feat_idx]\n",
    "    X_train_single = X_train_scaled[:, indices_single[:5]]\n",
    "    feature_names_single = [df_importancia.iloc[j]['Feature'] if j < len(df_importancia) else f'Feature_{j}' \n",
    "                           for j in indices_single[:5]]\n",
    "    \n",
    "    arbol_single = DecisionTreeClassifier(max_depth=4, random_state=42, min_samples_split=100)\n",
    "    arbol_single.fit(X_train_single, y_train)\n",
    "    \n",
    "    fig = plt.figure(figsize=(22, 12))\n",
    "    plot_tree(\n",
    "        arbol_single,\n",
    "        feature_names=feature_names_single,\n",
    "        class_names=['Clase 0', 'Clase 1'],\n",
    "        filled=True,\n",
    "        rounded=True,\n",
    "        fontsize=9\n",
    "    )\n",
    "    plt.title(f'Árbol #{i+1} - Variable Principal: {feat_name}\\nImportancia: {row[\"Importance\"]:.4f} ({row[\"Importance\"]/df_importancia[\"Importance\"].sum()*100:.2f}%)', \n",
    "              fontsize=13, fontweight='bold', pad=20)\n",
    "    \n",
    "    ruta_arbol_single = os.path.join(output_dir, f'arbol_decision_var_{feat_name}.png')\n",
    "    plt.savefig(ruta_arbol_single, dpi=300, bbox_inches='tight')\n",
    "    print(f'✓ Árbol para {feat_name} guardado en: {ruta_arbol_single}', flush=True)\n",
    "    plt.close()\n",
    "\n",
    "print('\\n✓ Todos los árboles de decisión individuales generados exitosamente', flush=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('=' * 80)\n",
    "print('PASO 13: VISUALIZACIONES EDA INDIVIDUALES')\n",
    "print('=' * 80)\n",
    "\n",
    "print('\\nCargando datos originales para EDA...', flush=True)\n",
    "\n",
    "archivo = 'Total_Mes_Act_Datos completos CORREGIDO.csv'\n",
    "df_eda = pd.read_csv(\n",
    "    archivo, \n",
    "    low_memory=False,\n",
    "    encoding='utf-8',\n",
    "    on_bad_lines='skip',\n",
    "    sep=';',\n",
    "    quotechar='\"'\n",
    ")\n",
    "\n",
    "print(f'✓ Datos cargados: {df_eda.shape[0]} filas, {df_eda.shape[1]} columnas', flush=True)\n",
    "\n",
    "columna_tarifa = 'TARIFA_NBO'\n",
    "columna_rentabilizacion = 'Rentabilizo'\n",
    "\n",
    "df_eda['variable_objetivo'] = np.where(\n",
    "    (df_eda[columna_tarifa].notna()) & \n",
    "    (df_eda[columna_rentabilizacion].notna()),\n",
    "    1,\n",
    "    0\n",
    ")\n",
    "\n",
    "distribucion = df_eda['variable_objetivo'].value_counts()\n",
    "\n",
    "print('\\nGenerando visualizaciones individuales...', flush=True)\n",
    "\n",
    "# 1. Distribución de Variable Objetivo (Barras)\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "distribucion.plot(kind='bar', color=['coral', 'steelblue'], ax=ax)\n",
    "ax.set_title('Distribución de Variable Objetivo', fontsize=14, fontweight='bold', pad=20)\n",
    "ax.set_xlabel('Clase (0=No cumple, 1=Cumple)', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Frecuencia', fontsize=12, fontweight='bold')\n",
    "ax.set_xticklabels(['No cumple', 'Cumple'], rotation=0)\n",
    "for i, v in enumerate(distribucion.values):\n",
    "    ax.text(i, v + max(distribucion.values) * 0.01, f'{v:,}', \n",
    "            ha='center', va='bottom', fontweight='bold', fontsize=11)\n",
    "plt.tight_layout()\n",
    "ruta = os.path.join(output_dir, 'eda_01_distribucion_variable_objetivo.png')\n",
    "plt.savefig(ruta, dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(f'✓ 1. Distribución variable objetivo guardada en: {ruta}', flush=True)\n",
    "\n",
    "# 2. Distribución Porcentual (Pastel)\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "distribucion.plot(kind='pie', autopct='%1.1f%%', colors=['coral', 'steelblue'], ax=ax, startangle=90)\n",
    "ax.set_title('Distribución Porcentual de Variable Objetivo', fontsize=14, fontweight='bold', pad=20)\n",
    "ax.set_ylabel('')\n",
    "plt.tight_layout()\n",
    "ruta = os.path.join(output_dir, 'eda_02_distribucion_porcentual.png')\n",
    "plt.savefig(ruta, dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(f'✓ 2. Distribución porcentual guardada en: {ruta}', flush=True)\n",
    "\n",
    "# 3. Top 15 Columnas con Valores Faltantes\n",
    "missing = df_eda.isnull().sum()\n",
    "missing_pct = (missing / len(df_eda)) * 100\n",
    "missing_df = pd.DataFrame({\n",
    "    'Valores_Faltantes': missing,\n",
    "    'Porcentaje': missing_pct\n",
    "})\n",
    "missing_df = missing_df[missing_df['Valores_Faltantes'] > 0].sort_values('Porcentaje', ascending=False).head(15)\n",
    "\n",
    "if len(missing_df) > 0:\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    missing_df['Porcentaje'].plot(kind='barh', color='orange', ax=ax)\n",
    "    ax.set_title('Top 15 Columnas con Valores Faltantes', fontsize=14, fontweight='bold', pad=20)\n",
    "    ax.set_xlabel('Porcentaje de Valores Faltantes', fontsize=12, fontweight='bold')\n",
    "    ax.set_ylabel('Columna', fontsize=12, fontweight='bold')\n",
    "    ax.invert_yaxis()\n",
    "    for i, v in enumerate(missing_df['Porcentaje'].values):\n",
    "        ax.text(v + 0.5, i, f'{v:.2f}%', va='center', fontweight='bold', fontsize=9)\n",
    "    plt.tight_layout()\n",
    "    ruta = os.path.join(output_dir, 'eda_03_valores_faltantes.png')\n",
    "    plt.savefig(ruta, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f'✓ 3. Valores faltantes guardada en: {ruta}', flush=True)\n",
    "\n",
    "# 4. Boxplots de Variables Numéricas\n",
    "columnas_numericas = df_eda.select_dtypes(include=[np.number]).columns.tolist()\n",
    "if len(columnas_numericas) > 0:\n",
    "    df_numericas = df_eda[columnas_numericas[:6]].copy()\n",
    "    fig, ax = plt.subplots(figsize=(14, 8))\n",
    "    df_numericas.boxplot(ax=ax, rot=45)\n",
    "    ax.set_title('Distribución de Variables Numéricas (Top 6)', fontsize=14, fontweight='bold', pad=20)\n",
    "    ax.set_ylabel('Valor', fontsize=12, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    ruta = os.path.join(output_dir, 'eda_04_boxplots_variables_numericas.png')\n",
    "    plt.savefig(ruta, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f'✓ 4. Boxplots variables numéricas guardada en: {ruta}', flush=True)\n",
    "\n",
    "# 5. Histogramas de Variables Numéricas\n",
    "if len(columnas_numericas) > 0:\n",
    "    df_numericas = df_eda[columnas_numericas[:6]].copy()\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    axes = axes.flatten()\n",
    "    for i, col in enumerate(df_numericas.columns):\n",
    "        if i < len(axes):\n",
    "            df_numericas[col].hist(bins=30, ax=axes[i], color='steelblue', edgecolor='black')\n",
    "            axes[i].set_title(f'{col}', fontsize=11, fontweight='bold')\n",
    "            axes[i].set_xlabel('Valor', fontsize=10)\n",
    "            axes[i].set_ylabel('Frecuencia', fontsize=10)\n",
    "            axes[i].grid(alpha=0.3)\n",
    "    plt.suptitle('Histogramas de Variables Numéricas (Top 6)', fontsize=14, fontweight='bold', y=0.995)\n",
    "    plt.tight_layout()\n",
    "    ruta = os.path.join(output_dir, 'eda_05_histogramas_variables_numericas.png')\n",
    "    plt.savefig(ruta, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f'✓ 5. Histogramas variables numéricas guardada en: {ruta}', flush=True)\n",
    "\n",
    "# 6. Distribución de TARIFA_NBO\n",
    "if columna_tarifa in df_eda.columns:\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    df_eda[columna_tarifa].notna().value_counts().plot(kind='bar', color=['red', 'green'], ax=ax)\n",
    "    ax.set_title(f'Distribución de {columna_tarifa}', fontsize=14, fontweight='bold', pad=20)\n",
    "    ax.set_xlabel('Tiene Valor', fontsize=12, fontweight='bold')\n",
    "    ax.set_ylabel('Frecuencia', fontsize=12, fontweight='bold')\n",
    "    ax.set_xticklabels(['Faltante', 'Presente'], rotation=0)\n",
    "    for i, v in enumerate(df_eda[columna_tarifa].notna().value_counts().values):\n",
    "        ax.text(i, v + max(df_eda[columna_tarifa].notna().value_counts().values) * 0.01, f'{v:,}', \n",
    "                ha='center', va='bottom', fontweight='bold', fontsize=11)\n",
    "    plt.tight_layout()\n",
    "    ruta = os.path.join(output_dir, 'eda_06_distribucion_tarifa_nbo.png')\n",
    "    plt.savefig(ruta, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f'✓ 6. Distribución TARIFA_NBO guardada en: {ruta}', flush=True)\n",
    "\n",
    "# 7. Distribución de Rentabilizo\n",
    "if columna_rentabilizacion in df_eda.columns:\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    df_eda[columna_rentabilizacion].notna().value_counts().plot(kind='bar', color=['red', 'green'], ax=ax)\n",
    "    ax.set_title(f'Distribución de {columna_rentabilizacion}', fontsize=14, fontweight='bold', pad=20)\n",
    "    ax.set_xlabel('Tiene Valor', fontsize=12, fontweight='bold')\n",
    "    ax.set_ylabel('Frecuencia', fontsize=12, fontweight='bold')\n",
    "    ax.set_xticklabels(['Faltante', 'Presente'], rotation=0)\n",
    "    for i, v in enumerate(df_eda[columna_rentabilizacion].notna().value_counts().values):\n",
    "        ax.text(i, v + max(df_eda[columna_rentabilizacion].notna().value_counts().values) * 0.01, f'{v:,}', \n",
    "                ha='center', va='bottom', fontweight='bold', fontsize=11)\n",
    "    plt.tight_layout()\n",
    "    ruta = os.path.join(output_dir, 'eda_07_distribucion_rentabilizo.png')\n",
    "    plt.savefig(ruta, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f'✓ 7. Distribución Rentabilizo guardada en: {ruta}', flush=True)\n",
    "\n",
    "# 8. Distribución de Tipos de Datos\n",
    "tipos_datos = df_eda.dtypes.value_counts()\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "tipos_datos.plot(kind='bar', color='purple', ax=ax)\n",
    "ax.set_title('Distribución de Tipos de Datos', fontsize=14, fontweight='bold', pad=20)\n",
    "ax.set_xlabel('Tipo de Dato', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Cantidad de Columnas', fontsize=12, fontweight='bold')\n",
    "ax.tick_params(axis='x', rotation=45)\n",
    "for i, v in enumerate(tipos_datos.values):\n",
    "    ax.text(i, v + max(tipos_datos.values) * 0.01, str(v), \n",
    "            ha='center', va='bottom', fontweight='bold', fontsize=11)\n",
    "plt.tight_layout()\n",
    "ruta = os.path.join(output_dir, 'eda_08_distribucion_tipos_datos.png')\n",
    "plt.savefig(ruta, dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(f'✓ 8. Distribución tipos de datos guardada en: {ruta}', flush=True)\n",
    "\n",
    "# 9. Resumen Estadístico del Dataset\n",
    "resumen_stats = {\n",
    "    'Total Filas': len(df_eda),\n",
    "    'Total Columnas': len(df_eda.columns),\n",
    "    'Columnas Numéricas': len(columnas_numericas),\n",
    "    'Columnas Categóricas': len(df_eda.columns) - len(columnas_numericas),\n",
    "    'Clase 1 (Cumple)': int(distribucion.get(1, 0)),\n",
    "    'Clase 0 (No cumple)': int(distribucion.get(0, 0))\n",
    "}\n",
    "stats_df = pd.DataFrame(list(resumen_stats.items()), columns=['Métrica', 'Valor'])\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "stats_df.set_index('Métrica')['Valor'].plot(kind='barh', color='teal', ax=ax)\n",
    "ax.set_title('Resumen Estadístico del Dataset', fontsize=14, fontweight='bold', pad=20)\n",
    "ax.set_xlabel('Valor', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Métrica', fontsize=12, fontweight='bold')\n",
    "ax.invert_yaxis()\n",
    "for i, v in enumerate(stats_df['Valor'].values):\n",
    "    ax.text(v + max(stats_df['Valor'].values) * 0.01, i, f'{v:,}', \n",
    "            va='center', fontweight='bold', fontsize=11)\n",
    "plt.tight_layout()\n",
    "ruta = os.path.join(output_dir, 'eda_09_resumen_estadistico.png')\n",
    "plt.savefig(ruta, dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(f'✓ 9. Resumen estadístico guardada en: {ruta}', flush=True)\n",
    "\n",
    "print('\\n✓ Paso 13 completado exitosamente')\n",
    "print(f'\\nTodas las visualizaciones EDA individuales están guardadas en: {output_dir}/', flush=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paso 14: Comparación de Modelos (Regresión Logística, Random Forest, XGBoost)\n",
    "\n",
    "En este paso entrenamos los tres modelos principales usando los datos donde TARIFA_NBO y Rentabilizo tienen información, generamos visualizaciones comparativas y validamos cuál modelo es el mejor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('=' * 80)\n",
    "print('PASO 14: COMPARACIÓN DE MODELOS')\n",
    "print('=' * 80)\n",
    "\n",
    "# Crear subcarpeta para archivos del paso 14\n",
    "output_dir_paso14 = os.path.join(output_dir, 'paso14')\n",
    "os.makedirs(output_dir_paso14, exist_ok=True)\n",
    "print(f'\\n✓ Subcarpeta creada: {output_dir_paso14}', flush=True)\n",
    "\n",
    "print('\\nCargando y preparando datos...', flush=True)\n",
    "\n",
    "archivo = 'Total_Mes_Act_Datos completos CORREGIDO.csv'\n",
    "df = pd.read_csv(\n",
    "    archivo, \n",
    "    low_memory=False,\n",
    "    encoding='utf-8',\n",
    "    on_bad_lines='skip',\n",
    "    sep=';',\n",
    "    quotechar='\"'\n",
    ")\n",
    "\n",
    "print(f'✓ Datos cargados: {df.shape[0]} filas, {df.shape[1]} columnas', flush=True)\n",
    "\n",
    "columna_tarifa = 'TARIFA_NBO'\n",
    "columna_rentabilizacion = 'Rentabilizo'\n",
    "\n",
    "df['variable_objetivo'] = np.where(\n",
    "    (df[columna_tarifa].notna()) & \n",
    "    (df[columna_rentabilizacion].notna()),\n",
    "    1,\n",
    "    0\n",
    ")\n",
    "\n",
    "y = df['variable_objetivo'].copy()\n",
    "X = df.drop(columns=['variable_objetivo', columna_tarifa, columna_rentabilizacion])\n",
    "\n",
    "print(f'\\nVariable objetivo creada:', flush=True)\n",
    "print(f'  Clase 0 (No cumple): {sum(y == 0):,} registros ({sum(y == 0)/len(y)*100:.2f}%)', flush=True)\n",
    "print(f'  Clase 1 (Cumple): {sum(y == 1):,} registros ({sum(y == 1)/len(y)*100:.2f}%)', flush=True)\n",
    "\n",
    "columnas_numericas = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "columnas_categoricas = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "X_processed = X[columnas_numericas].copy()\n",
    "\n",
    "print(f'\\nProcesando {len(columnas_categoricas)} columnas categóricas...', flush=True)\n",
    "for i, col in enumerate(columnas_categoricas):\n",
    "    if X[col].nunique() < 50:\n",
    "        le = LabelEncoder()\n",
    "        X_processed[col] = le.fit_transform(X[col].astype(str).fillna('Missing'))\n",
    "    if (i + 1) % 10 == 0:\n",
    "        print(f'  Procesadas {i + 1}/{len(columnas_categoricas)} columnas...', flush=True)\n",
    "\n",
    "X_processed = X_processed.fillna(X_processed.median())\n",
    "\n",
    "# Guardar nombres de columnas originales para mapeo\n",
    "feature_names = list(X_processed.columns)\n",
    "\n",
    "print(f'\\n✓ Datos procesados: {X_processed.shape[1]} features', flush=True)\n",
    "print(f'✓ Nombres de features guardados para mapeo', flush=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_processed, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f'✓ División train/test:', flush=True)\n",
    "print(f'  Train: {X_train.shape[0]:,} muestras', flush=True)\n",
    "print(f'  Test: {X_test.shape[0]:,} muestras', flush=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14.1: Entrenamiento de Modelos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, \n",
    "                            f1_score, roc_auc_score, confusion_matrix, \n",
    "                            classification_report, roc_curve, auc, \n",
    "                            precision_recall_curve)\n",
    "\n",
    "print('\\n' + '=' * 80)\n",
    "print('ENTRENAMIENTO DE MODELOS')\n",
    "print('=' * 80)\n",
    "\n",
    "modelos = {}\n",
    "resultados_modelos = {}\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print('\\n1. Regresión Logística...', flush=True)\n",
    "lr = LogisticRegression(max_iter=1000, random_state=42, n_jobs=-1)\n",
    "lr.fit(X_train_scaled, y_train)\n",
    "y_pred_lr = lr.predict(X_test_scaled)\n",
    "y_pred_proba_lr = lr.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "modelos['LogisticRegression'] = lr\n",
    "resultados_modelos['LogisticRegression'] = {\n",
    "    'y_pred': y_pred_lr,\n",
    "    'y_pred_proba': y_pred_proba_lr,\n",
    "    'scaler': scaler\n",
    "}\n",
    "print('  ✓ Regresión Logística entrenada', flush=True)\n",
    "\n",
    "print('\\n2. Random Forest...', flush=True)\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1, max_depth=20)\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "y_pred_proba_rf = rf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "modelos['RandomForest'] = rf\n",
    "resultados_modelos['RandomForest'] = {\n",
    "    'y_pred': y_pred_rf,\n",
    "    'y_pred_proba': y_pred_proba_rf,\n",
    "    'scaler': None\n",
    "}\n",
    "print('  ✓ Random Forest entrenado', flush=True)\n",
    "\n",
    "print('\\n3. XGBoost...', flush=True)\n",
    "xgb = XGBClassifier(n_estimators=100, max_depth=5, learning_rate=0.1, \n",
    "                    random_state=42, n_jobs=-1, eval_metric='logloss')\n",
    "xgb.fit(X_train, y_train)\n",
    "y_pred_xgb = xgb.predict(X_test)\n",
    "y_pred_proba_xgb = xgb.predict_proba(X_test)[:, 1]\n",
    "\n",
    "modelos['XGBoost'] = xgb\n",
    "resultados_modelos['XGBoost'] = {\n",
    "    'y_pred': y_pred_xgb,\n",
    "    'y_pred_proba': y_pred_proba_xgb,\n",
    "    'scaler': None\n",
    "}\n",
    "print('  ✓ XGBoost entrenado', flush=True)\n",
    "\n",
    "print('\\n✓ Todos los modelos entrenados exitosamente', flush=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14.2: Cálculo de Métricas y Comparación\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n' + '=' * 80)\n",
    "print('CÁLCULO DE MÉTRICAS')\n",
    "print('=' * 80)\n",
    "\n",
    "metricas_comparacion = []\n",
    "\n",
    "for nombre, modelo in modelos.items():\n",
    "    y_pred = resultados_modelos[nombre]['y_pred']\n",
    "    y_pred_proba = resultados_modelos[nombre]['y_pred_proba']\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    \n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "    precision_curve, recall_curve, _ = precision_recall_curve(y_test, y_pred_proba)\n",
    "    pr_auc = auc(recall_curve, precision_curve)\n",
    "    \n",
    "    metricas_comparacion.append({\n",
    "        'Modelo': nombre,\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1-Score': f1,\n",
    "        'ROC-AUC': roc_auc,\n",
    "        'PR-AUC': pr_auc\n",
    "    })\n",
    "    \n",
    "    print(f'\\n{nombre}:', flush=True)\n",
    "    print(f'  Accuracy:  {accuracy:.4f}', flush=True)\n",
    "    print(f'  Precision: {precision:.4f}', flush=True)\n",
    "    print(f'  Recall:    {recall:.4f}', flush=True)\n",
    "    print(f'  F1-Score:  {f1:.4f}', flush=True)\n",
    "    print(f'  ROC-AUC:   {roc_auc:.4f}', flush=True)\n",
    "    print(f'  PR-AUC:    {pr_auc:.4f}', flush=True)\n",
    "\n",
    "df_metricas = pd.DataFrame(metricas_comparacion)\n",
    "df_metricas.to_csv(os.path.join(output_dir_paso14, 'comparacion_modelos_metricas.csv'), index=False)\n",
    "print(f'\\n✓ Métricas guardadas en: comparacion_modelos_metricas.csv', flush=True)\n",
    "\n",
    "mejor_modelo_accuracy = df_metricas.loc[df_metricas['Accuracy'].idxmax(), 'Modelo']\n",
    "mejor_modelo_f1 = df_metricas.loc[df_metricas['F1-Score'].idxmax(), 'Modelo']\n",
    "mejor_modelo_roc = df_metricas.loc[df_metricas['ROC-AUC'].idxmax(), 'Modelo']\n",
    "\n",
    "print('\\n' + '=' * 80)\n",
    "print('MEJOR MODELO POR MÉTRICA')\n",
    "print('=' * 80)\n",
    "print(f'  Mejor Accuracy:  {mejor_modelo_accuracy} ({df_metricas.loc[df_metricas[\"Accuracy\"].idxmax(), \"Accuracy\"]:.4f})', flush=True)\n",
    "print(f'  Mejor F1-Score:  {mejor_modelo_f1} ({df_metricas.loc[df_metricas[\"F1-Score\"].idxmax(), \"F1-Score\"]:.4f})', flush=True)\n",
    "print(f'  Mejor ROC-AUC:   {mejor_modelo_roc} ({df_metricas.loc[df_metricas[\"ROC-AUC\"].idxmax(), \"ROC-AUC\"]:.4f})', flush=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14.3: Visualizaciones Comparativas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n' + '=' * 80)\n",
    "print('GENERANDO VISUALIZACIONES COMPARATIVAS')\n",
    "print('=' * 80)\n",
    "\n",
    "fig = plt.figure(figsize=(20, 14))\n",
    "\n",
    "# 1. Comparación de Métricas (Barras)\n",
    "ax1 = plt.subplot(2, 3, 1)\n",
    "metricas_plot = df_metricas.set_index('Modelo')[['Accuracy', 'Precision', 'Recall', 'F1-Score']]\n",
    "metricas_plot.plot(kind='bar', ax=ax1, width=0.8)\n",
    "ax1.set_title('Comparación de Métricas Básicas', fontsize=12, fontweight='bold', pad=15)\n",
    "ax1.set_ylabel('Score', fontsize=10, fontweight='bold')\n",
    "ax1.set_xlabel('Modelo', fontsize=10, fontweight='bold')\n",
    "ax1.legend(loc='upper left', fontsize=9)\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 2. Comparación ROC-AUC y PR-AUC\n",
    "ax2 = plt.subplot(2, 3, 2)\n",
    "auc_plot = df_metricas.set_index('Modelo')[['ROC-AUC', 'PR-AUC']]\n",
    "auc_plot.plot(kind='bar', ax=ax2, width=0.8, color=['steelblue', 'coral'])\n",
    "ax2.set_title('Comparación ROC-AUC y PR-AUC', fontsize=12, fontweight='bold', pad=15)\n",
    "ax2.set_ylabel('AUC Score', fontsize=10, fontweight='bold')\n",
    "ax2.set_xlabel('Modelo', fontsize=10, fontweight='bold')\n",
    "ax2.legend(loc='upper left', fontsize=9)\n",
    "ax2.tick_params(axis='x', rotation=45)\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 3. Curvas ROC Comparativas\n",
    "ax3 = plt.subplot(2, 3, 3)\n",
    "for nombre in modelos.keys():\n",
    "    y_pred_proba = resultados_modelos[nombre]['y_pred_proba']\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    ax3.plot(fpr, tpr, lw=2, label=f'{nombre} (AUC = {roc_auc:.4f})')\n",
    "ax3.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random Classifier')\n",
    "ax3.set_xlabel('Tasa de Falsos Positivos', fontsize=10, fontweight='bold')\n",
    "ax3.set_ylabel('Tasa de Verdaderos Positivos', fontsize=10, fontweight='bold')\n",
    "ax3.set_title('Curvas ROC Comparativas', fontsize=12, fontweight='bold', pad=15)\n",
    "ax3.legend(loc=\"lower right\", fontsize=9)\n",
    "ax3.grid(alpha=0.3)\n",
    "\n",
    "# 4. Curvas Precision-Recall Comparativas\n",
    "ax4 = plt.subplot(2, 3, 4)\n",
    "for nombre in modelos.keys():\n",
    "    y_pred_proba = resultados_modelos[nombre]['y_pred_proba']\n",
    "    precision_curve, recall_curve, _ = precision_recall_curve(y_test, y_pred_proba)\n",
    "    pr_auc = auc(recall_curve, precision_curve)\n",
    "    ax4.plot(recall_curve, precision_curve, lw=2, label=f'{nombre} (AUC = {pr_auc:.4f})')\n",
    "baseline = len(y_test[y_test==1]) / len(y_test)\n",
    "ax4.axhline(y=baseline, color='r', linestyle='--', label=f'Baseline ({baseline:.4f})')\n",
    "ax4.set_xlabel('Recall', fontsize=10, fontweight='bold')\n",
    "ax4.set_ylabel('Precision', fontsize=10, fontweight='bold')\n",
    "ax4.set_title('Curvas Precision-Recall Comparativas', fontsize=12, fontweight='bold', pad=15)\n",
    "ax4.legend(loc=\"lower left\", fontsize=9)\n",
    "ax4.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "ruta_comparacion = os.path.join(output_dir_paso14, 'comparacion_modelos_visualizacion.png')\n",
    "plt.savefig(ruta_comparacion, dpi=300, bbox_inches='tight')\n",
    "print(f'\\n✓ Visualización comparativa guardada en: {ruta_comparacion}', flush=True)\n",
    "plt.close()\n",
    "\n",
    "# Matrices de Confusión en figura separada\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "for idx, nombre in enumerate(modelos.keys()):\n",
    "    y_pred = resultados_modelos[nombre]['y_pred']\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[idx], \n",
    "                xticklabels=['Clase 0', 'Clase 1'], \n",
    "                yticklabels=['Clase 0', 'Clase 1'])\n",
    "    axes[idx].set_title(f'Matriz Confusión - {nombre}', fontsize=12, fontweight='bold', pad=15)\n",
    "    axes[idx].set_xlabel('Predicción', fontsize=10, fontweight='bold')\n",
    "    axes[idx].set_ylabel('Real', fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "ruta_confusion = os.path.join(output_dir_paso14, 'comparacion_modelos_matrices_confusion.png')\n",
    "plt.savefig(ruta_confusion, dpi=300, bbox_inches='tight')\n",
    "print(f'✓ Matrices de confusión guardadas en: {ruta_confusion}', flush=True)\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14.4: Validación Cruzada y Selección del Mejor Modelo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n' + '=' * 80)\n",
    "print('VALIDACIÓN CRUZADA (5 FOLDS)')\n",
    "print('=' * 80)\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "resultados_cv = {}\n",
    "\n",
    "print('\\nRealizando validación cruzada para cada modelo...', flush=True)\n",
    "\n",
    "# Regresión Logística\n",
    "print('\\n1. Regresión Logística...', flush=True)\n",
    "cv_scores_lr = cross_val_score(lr, X_train_scaled, y_train, cv=cv, scoring='accuracy', n_jobs=-1)\n",
    "resultados_cv['LogisticRegression'] = {\n",
    "    'mean': cv_scores_lr.mean(),\n",
    "    'std': cv_scores_lr.std(),\n",
    "    'scores': cv_scores_lr.tolist()\n",
    "}\n",
    "print(f'  Accuracy CV: {cv_scores_lr.mean():.4f} (+/- {cv_scores_lr.std() * 2:.4f})', flush=True)\n",
    "\n",
    "# Random Forest\n",
    "print('\\n2. Random Forest...', flush=True)\n",
    "cv_scores_rf = cross_val_score(rf, X_train, y_train, cv=cv, scoring='accuracy', n_jobs=-1)\n",
    "resultados_cv['RandomForest'] = {\n",
    "    'mean': cv_scores_rf.mean(),\n",
    "    'std': cv_scores_rf.std(),\n",
    "    'scores': cv_scores_rf.tolist()\n",
    "}\n",
    "print(f'  Accuracy CV: {cv_scores_rf.mean():.4f} (+/- {cv_scores_rf.std() * 2:.4f})', flush=True)\n",
    "\n",
    "# XGBoost\n",
    "print('\\n3. XGBoost...', flush=True)\n",
    "cv_scores_xgb = cross_val_score(xgb, X_train, y_train, cv=cv, scoring='accuracy', n_jobs=-1)\n",
    "resultados_cv['XGBoost'] = {\n",
    "    'mean': cv_scores_xgb.mean(),\n",
    "    'std': cv_scores_xgb.std(),\n",
    "    'scores': cv_scores_xgb.tolist()\n",
    "}\n",
    "print(f'  Accuracy CV: {cv_scores_xgb.mean():.4f} (+/- {cv_scores_xgb.std() * 2:.4f})', flush=True)\n",
    "\n",
    "# Guardar resultados CV\n",
    "ruta_cv = os.path.join(output_dir_paso14, 'resultados_validacion_cruzada_paso14.json')\n",
    "with open(ruta_cv, 'w', encoding='utf-8') as f:\n",
    "    json.dump(resultados_cv, f, indent=2)\n",
    "print(f'\\n✓ Resultados de validación cruzada guardados en: {ruta_cv}', flush=True)\n",
    "\n",
    "# Visualización de resultados CV\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Boxplot de scores CV\n",
    "cv_data = [resultados_cv['LogisticRegression']['scores'],\n",
    "           resultados_cv['RandomForest']['scores'],\n",
    "           resultados_cv['XGBoost']['scores']]\n",
    "axes[0].boxplot(cv_data, labels=['Logistic\\nRegression', 'Random\\nForest', 'XGBoost'])\n",
    "axes[0].set_title('Distribución de Accuracy en Validación Cruzada', fontsize=14, fontweight='bold', pad=20)\n",
    "axes[0].set_ylabel('Accuracy', fontsize=12, fontweight='bold')\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Comparación de medias\n",
    "medias_cv = [resultados_cv['LogisticRegression']['mean'],\n",
    "             resultados_cv['RandomForest']['mean'],\n",
    "             resultados_cv['XGBoost']['mean']]\n",
    "stds_cv = [resultados_cv['LogisticRegression']['std'],\n",
    "           resultados_cv['RandomForest']['std'],\n",
    "           resultados_cv['XGBoost']['std']]\n",
    "axes[1].bar(['Logistic\\nRegression', 'Random\\nForest', 'XGBoost'], medias_cv, \n",
    "            yerr=[std * 2 for std in stds_cv], capsize=10, color=['steelblue', 'coral', 'green'], alpha=0.7)\n",
    "axes[1].set_title('Accuracy Promedio en Validación Cruzada', fontsize=14, fontweight='bold', pad=20)\n",
    "axes[1].set_ylabel('Accuracy', fontsize=12, fontweight='bold')\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "for i, (media, std) in enumerate(zip(medias_cv, stds_cv)):\n",
    "    axes[1].text(i, media + std * 2 + 0.01, f'{media:.4f}', \n",
    "                ha='center', va='bottom', fontweight='bold', fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "ruta_cv_viz = os.path.join(output_dir_paso14, 'validacion_cruzada_paso14.png')\n",
    "plt.savefig(ruta_cv_viz, dpi=300, bbox_inches='tight')\n",
    "print(f'✓ Visualización de validación cruzada guardada en: {ruta_cv_viz}', flush=True)\n",
    "plt.close()\n",
    "\n",
    "# Determinar mejor modelo\n",
    "mejor_modelo_cv = max(resultados_cv.items(), key=lambda x: x[1]['mean'])[0]\n",
    "mejor_score_cv = resultados_cv[mejor_modelo_cv]['mean']\n",
    "\n",
    "print('\\n' + '=' * 80)\n",
    "print('CONCLUSIÓN FINAL')\n",
    "print('=' * 80)\n",
    "print(f'\\nMejor modelo según Validación Cruzada: {mejor_modelo_cv}', flush=True)\n",
    "print(f'  Accuracy promedio: {mejor_score_cv:.4f} (+/- {resultados_cv[mejor_modelo_cv][\"std\"] * 2:.4f})', flush=True)\n",
    "\n",
    "print('\\nResumen de todos los modelos:', flush=True)\n",
    "for nombre, resultados in resultados_cv.items():\n",
    "    print(f'  {nombre:20s}: {resultados[\"mean\"]:.4f} (+/- {resultados[\"std\"] * 2:.4f})', flush=True)\n",
    "\n",
    "conclusion_final = {\n",
    "    'mejor_modelo_cv': mejor_modelo_cv,\n",
    "    'mejor_score_cv': mejor_score_cv,\n",
    "    'resultados_cv': resultados_cv,\n",
    "    'metricas_test': metricas_comparacion,\n",
    "    'mejor_modelo_accuracy': mejor_modelo_accuracy,\n",
    "    'mejor_modelo_f1': mejor_modelo_f1,\n",
    "    'mejor_modelo_roc': mejor_modelo_roc\n",
    "}\n",
    "\n",
    "ruta_conclusion = os.path.join(output_dir_paso14, 'conclusion_paso14.json')\n",
    "with open(ruta_conclusion, 'w', encoding='utf-8') as f:\n",
    "    json.dump(conclusion_final, f, indent=2)\n",
    "print(f'\\n✓ Conclusión final guardada en: {ruta_conclusion}', flush=True)\n",
    "\n",
    "print('\\n✓ Paso 14 completado exitosamente', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n' + '=' * 80)\n",
    "print('GENERANDO ÁRBOLES DE DECISIÓN CON VARIABLES REALES')\n",
    "print('=' * 80)\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "\n",
    "print('\\nGenerando árboles de decisión para Random Forest y XGBoost...', flush=True)\n",
    "print('(Nota: Logistic Regression es un modelo lineal, no genera árboles)', flush=True)\n",
    "\n",
    "# Obtener importancia de features para cada modelo\n",
    "print('\\n1. Analizando importancia de variables...', flush=True)\n",
    "\n",
    "# Random Forest - Importancia de features\n",
    "importancia_rf = rf.feature_importances_\n",
    "indices_rf = np.argsort(importancia_rf)[::-1]\n",
    "top_features_rf = indices_rf[:15]  # Top 15 features\n",
    "\n",
    "# XGBoost - Importancia de features\n",
    "importancia_xgb = xgb.feature_importances_\n",
    "indices_xgb = np.argsort(importancia_xgb)[::-1]\n",
    "top_features_xgb = indices_xgb[:15]  # Top 15 features\n",
    "\n",
    "print(f'  ✓ Top 15 features identificados para Random Forest', flush=True)\n",
    "print(f'  ✓ Top 15 features identificados para XGBoost', flush=True)\n",
    "\n",
    "# Función para obtener nombres reales de features\n",
    "def obtener_nombres_reales(indices, feature_names):\n",
    "    return [feature_names[i] if i < len(feature_names) else f'Feature_{i}' for i in indices]\n",
    "\n",
    "# Generar árboles para Random Forest\n",
    "print('\\n2. Generando árboles de decisión para Random Forest...', flush=True)\n",
    "\n",
    "# Árbol individual de Random Forest (primer árbol del ensemble)\n",
    "arbol_rf_individual = rf.estimators_[0]  # Primer árbol del Random Forest\n",
    "\n",
    "# Árbol completo del Random Forest (usando todos los features importantes)\n",
    "profundidades_rf = [3, 4, 5]\n",
    "for depth in profundidades_rf:\n",
    "    # Usar top features para crear un árbol más interpretable\n",
    "    if isinstance(X_train, pd.DataFrame):\n",
    "        X_train_top_rf = X_train.iloc[:, top_features_rf[:10]]\n",
    "    else:\n",
    "        X_train_top_rf = X_train[:, top_features_rf[:10]]\n",
    "    feature_names_top_rf = obtener_nombres_reales(top_features_rf[:10], feature_names)\n",
    "    \n",
    "    arbol_rf_surrogado = DecisionTreeClassifier(max_depth=depth, random_state=42, min_samples_split=100)\n",
    "    arbol_rf_surrogado.fit(X_train_top_rf, y_train)\n",
    "    \n",
    "    fig = plt.figure(figsize=(24, 14))\n",
    "    plot_tree(\n",
    "        arbol_rf_surrogado,\n",
    "        feature_names=feature_names_top_rf,\n",
    "        class_names=['Clase 0\\n(No cumple)', 'Clase 1\\n(Cumple)'],\n",
    "        filled=True,\n",
    "        rounded=True,\n",
    "        fontsize=9,\n",
    "        proportion=True\n",
    "    )\n",
    "    plt.title(f'Árbol de Decisión - Random Forest - Profundidad {depth}\\nTop 10 Variables Más Importantes', \n",
    "              fontsize=14, fontweight='bold', pad=20)\n",
    "    \n",
    "    ruta_arbol = os.path.join(output_dir_paso14, f'arbol_random_forest_depth{depth}.png')\n",
    "    plt.savefig(ruta_arbol, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f'  ✓ Árbol Random Forest (profundidad {depth}) guardado en: {ruta_arbol}', flush=True)\n",
    "\n",
    "# Árbol individual del primer estimador de Random Forest\n",
    "print('\\n3. Generando árbol individual del primer estimador de Random Forest...', flush=True)\n",
    "fig = plt.figure(figsize=(24, 14))\n",
    "plot_tree(\n",
    "    arbol_rf_individual,\n",
    "    feature_names=feature_names,\n",
    "    class_names=['Clase 0\\n(No cumple)', 'Clase 1\\n(Cumple)'],\n",
    "    filled=True,\n",
    "    rounded=True,\n",
    "    fontsize=8,\n",
    "    max_depth=4  # Limitar profundidad para legibilidad\n",
    ")\n",
    "plt.title('Árbol Individual - Random Forest (Primer Estimador)\\nProfundidad limitada a 4 niveles', \n",
    "          fontsize=14, fontweight='bold', pad=20)\n",
    "ruta_arbol_rf_individual = os.path.join(output_dir_paso14, 'arbol_random_forest_individual.png')\n",
    "plt.savefig(ruta_arbol_rf_individual, dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(f'  ✓ Árbol individual guardado en: {ruta_arbol_rf_individual}', flush=True)\n",
    "\n",
    "# Generar árboles para XGBoost\n",
    "print('\\n4. Generando árboles de decisión para XGBoost...', flush=True)\n",
    "\n",
    "# XGBoost tiene árboles individuales, pero son más complejos\n",
    "# Crearemos árboles surrogados basados en las variables más importantes\n",
    "profundidades_xgb = [3, 4, 5]\n",
    "for depth in profundidades_xgb:\n",
    "    # Usar top features para crear un árbol más interpretable\n",
    "    if isinstance(X_train, pd.DataFrame):\n",
    "        X_train_top_xgb = X_train.iloc[:, top_features_xgb[:10]]\n",
    "    else:\n",
    "        X_train_top_xgb = X_train[:, top_features_xgb[:10]]\n",
    "    feature_names_top_xgb = obtener_nombres_reales(top_features_xgb[:10], feature_names)\n",
    "    \n",
    "    arbol_xgb_surrogado = DecisionTreeClassifier(max_depth=depth, random_state=42, min_samples_split=100)\n",
    "    arbol_xgb_surrogado.fit(X_train_top_xgb, y_train)\n",
    "    \n",
    "    fig = plt.figure(figsize=(24, 14))\n",
    "    plot_tree(\n",
    "        arbol_xgb_surrogado,\n",
    "        feature_names=feature_names_top_xgb,\n",
    "        class_names=['Clase 0\\n(No cumple)', 'Clase 1\\n(Cumple)'],\n",
    "        filled=True,\n",
    "        rounded=True,\n",
    "        fontsize=9,\n",
    "        proportion=True\n",
    "    )\n",
    "    plt.title(f'Árbol de Decisión - XGBoost - Profundidad {depth}\\nTop 10 Variables Más Importantes', \n",
    "              fontsize=14, fontweight='bold', pad=20)\n",
    "    \n",
    "    ruta_arbol = os.path.join(output_dir_paso14, f'arbol_xgboost_depth{depth}.png')\n",
    "    plt.savefig(ruta_arbol, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f'  ✓ Árbol XGBoost (profundidad {depth}) guardado en: {ruta_arbol}', flush=True)\n",
    "\n",
    "# Guardar mapeo de features\n",
    "mapeo_features = {\n",
    "    'feature_names': feature_names,\n",
    "    'top_features_rf': top_features_rf[:15].tolist(),\n",
    "    'top_features_xgb': top_features_xgb[:15].tolist(),\n",
    "    'nombres_top_rf': obtener_nombres_reales(top_features_rf[:15], feature_names),\n",
    "    'nombres_top_xgb': obtener_nombres_reales(top_features_xgb[:15], feature_names)\n",
    "}\n",
    "\n",
    "ruta_mapeo = os.path.join(output_dir_paso14, 'mapeo_features_paso14.json')\n",
    "with open(ruta_mapeo, 'w', encoding='utf-8') as f:\n",
    "    json.dump(mapeo_features, f, indent=2)\n",
    "print(f'\\n✓ Mapeo de features guardado en: {ruta_mapeo}', flush=True)\n",
    "\n",
    "# Guardar importancia de features\n",
    "df_importancia_rf = pd.DataFrame({\n",
    "    'Feature_Index': top_features_rf[:15],\n",
    "    'Feature_Name': obtener_nombres_reales(top_features_rf[:15], feature_names),\n",
    "    'Importance': importancia_rf[top_features_rf[:15]]\n",
    "})\n",
    "df_importancia_rf.to_csv(os.path.join(output_dir_paso14, 'importancia_features_random_forest.csv'), index=False)\n",
    "\n",
    "df_importancia_xgb = pd.DataFrame({\n",
    "    'Feature_Index': top_features_xgb[:15],\n",
    "    'Feature_Name': obtener_nombres_reales(top_features_xgb[:15], feature_names),\n",
    "    'Importance': importancia_xgb[top_features_xgb[:15]]\n",
    "})\n",
    "df_importancia_xgb.to_csv(os.path.join(output_dir_paso14, 'importancia_features_xgboost.csv'), index=False)\n",
    "\n",
    "print(f'✓ Importancia de features guardada en CSV', flush=True)\n",
    "\n",
    "print('\\n' + '=' * 80)\n",
    "print('RESUMEN DE ÁRBOLES GENERADOS')\n",
    "print('=' * 80)\n",
    "print(f'\\nRandom Forest:')\n",
    "print(f'  - Árbol individual (profundidad 4): arbol_random_forest_individual.png')\n",
    "for depth in profundidades_rf:\n",
    "    print(f'  - Árbol surrogado (profundidad {depth}): arbol_random_forest_depth{depth}.png')\n",
    "\n",
    "print(f'\\nXGBoost:')\n",
    "for depth in profundidades_xgb:\n",
    "    print(f'  - Árbol surrogado (profundidad {depth}): arbol_xgboost_depth{depth}.png')\n",
    "\n",
    "print(f'\\n✓ Todos los árboles guardados en: {output_dir_paso14}/', flush=True)\n",
    "print('\\n✓ Generación de árboles completada exitosamente', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
